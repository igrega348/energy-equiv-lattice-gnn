{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if os.path.abspath('../') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../'))\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from lattices.lattices import Catalogue\n",
    "from gnn.datasets import GLAMM_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble datasets for ML\n",
    "\n",
    "Training datasets in the paper are:\n",
    "\n",
    "| Dataset   | Base lattices | Imperfection levels (%) | \\# imp. per level  | Total lattices    |\n",
    "| --------- | ------------- | --------------------- | -- | ----------------- |\n",
    "| 0 imp quarter | 1750         | 0                   | 1 | 1750                 |\n",
    "| 0 imp half    | 3500         | 0                   | 1  | 3500                 |\n",
    "| 0 imp     | 7000           | 0                    | 1 | 7000                 |\n",
    "| 1 imp     | 7000            | 0, 2, 4, 7     | 1 | 27847                 |\n",
    "| 2 imp     | 7000             | 0, 2, 4, 7      | 2 |          48681                 |\n",
    "| 4 imp     | 7000             | 0, 2, 4, 7      | 4 |          90336                 |\n",
    "\n",
    "\n",
    "Test dataset is \n",
    "| Dataset   | Base lattices | Imperfection levels (%)  | \\# imp. per level  | Total lattices    |\n",
    "| --------- | ------------- | --------------------- | -- | ----------------- |\n",
    "| 0 imp     | 1296           | 10                    | 3 | 3888                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path('C:/temp/') # location where `.lat` catalogue files are stored\n",
    "assert input_dir.exists(), f\"Directory {input_dir} does not exist.\"\n",
    "output_dir = Path('../datasets')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "make_split = False # create train/val/test split now or load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_dir: Path, max_imp: int = 1, regex: Optional[str] = None) -> Tuple[dict, dict]:\n",
    "    n_2_imp = lambda x: float(x.split('_')[4]) # extract imperfection from lattice name\n",
    "    num_imp = {}\n",
    "    selected_data = {}\n",
    "\n",
    "    cat_files = list(input_dir.glob('*.lat'))\n",
    "    print(f'Found {len(cat_files)} catalogue files')\n",
    "    for f in tqdm(cat_files):\n",
    "        cat = Catalogue.from_file(f, indexing=0, regex=regex)\n",
    "        for data in cat:\n",
    "            name = data['name']\n",
    "            base_name = Catalogue.n_2_bn(name)\n",
    "            imp = n_2_imp(name)\n",
    "            if base_name not in num_imp:\n",
    "                num_imp[base_name] = {}\n",
    "            if imp not in num_imp[base_name]:\n",
    "                num_imp[base_name][imp] = 0\n",
    "            else:\n",
    "                if num_imp[base_name][imp] >= max_imp:\n",
    "                    continue\n",
    "\n",
    "            selected_data[name] = data\n",
    "\n",
    "            num_imp[base_name][imp] += 1\n",
    "    \n",
    "    return selected_data, num_imp\n",
    "\n",
    "def load_names(input_dir: Path) -> set:\n",
    "    cat_files = list(input_dir.glob('*.lat'))\n",
    "    print(f'Found {len(cat_files)} catalogue files')\n",
    "    names = set()\n",
    "    for f in tqdm(cat_files):\n",
    "        names = names.union(set(Catalogue.get_names(f)))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_split:\n",
    "    all_names = load_names(input_dir)\n",
    "    base_names = list(set([Catalogue.n_2_bn(name) for name in all_names]))\n",
    "    # split base names into train and validation. Take 7000 for train and rest for validation\n",
    "    np.random.shuffle(base_names)\n",
    "    train_base_names = set(base_names[:7000])\n",
    "    val_base_names = set(base_names[7000:])\n",
    "    # sort by code and save to file\n",
    "    train_code_map = {name.split('_')[2]: Catalogue.n_2_bn(name) for name in train_base_names}\n",
    "    val_code_map = {name.split('_')[2]: Catalogue.n_2_bn(name) for name in val_base_names}\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_dir/'train_base_names.txt', 'w') as f:\n",
    "        sorted_names = sorted(train_code_map.keys(), key=lambda x: int(x[1:]))\n",
    "        f.write('\\n'.join([train_code_map[name] for name in sorted_names]))\n",
    "    with open(output_dir/'val_base_names.txt', 'w') as f:\n",
    "        sorted_names = sorted(val_code_map.keys(), key=lambda x: int(x[1:]))\n",
    "        f.write('\\n'.join([val_code_map[name] for name in sorted_names]))\n",
    "else:\n",
    "    # load train and validation names from files\n",
    "    train_base_names = set(pd.read_csv('./train_base_names.txt', header=None)[0].values)\n",
    "    val_base_names = set(pd.read_csv('./val_base_names.txt', header=None)[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 imp (imperfection level 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 catalogue files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:15<00:00, 15.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base names: 8954\n",
      "8954\n",
      "Unit cell catalogue with 7000 entries\n",
      "Unit cell catalogue with 1296 entries\n"
     ]
    }
   ],
   "source": [
    "selected_data, num_imp = load_data(input_dir, max_imp=1, regex='.*p_0.0_.*')\n",
    "\n",
    "print(f'Number of base names: {len(num_imp)}')\n",
    "print(len(selected_data))\n",
    "\n",
    "training_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in train_base_names}\n",
    "validation_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in val_base_names}\n",
    "training_cat = Catalogue.from_dict(training_dict)\n",
    "validation_cat = Catalogue.from_dict(validation_dict)\n",
    "print(training_cat)\n",
    "print(validation_cat)\n",
    "Path(output_dir/'0imp/raw').mkdir(parents=True, exist_ok=True)\n",
    "training_cat.to_file(output_dir/'0imp/raw/training_cat.lat')\n",
    "validation_cat.to_file(output_dir/'0imp/raw/validation_cat.lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset can be converted to pytorch now or later when it is loaded from `.lat` file in the `raw` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data and save as pt files\n",
    "train_dset = GLAMM_Dataset(\n",
    "    output_dir/'0imp', './training_cat.lat', 'train.pt', graph_ft_format='cartesian_4', n_reldens=3\n",
    ")\n",
    "val_dset = GLAMM_Dataset(\n",
    "    output_dir/'0imp', './validation_cat.lat', 'valid.pt', graph_ft_format='cartesian_4', n_reldens=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 imp half (imperfection level 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 catalogue files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:13<00:00, 14.70s/it]\n"
     ]
    }
   ],
   "source": [
    "Path(output_dir/'0imp_half/raw').mkdir(parents=True, exist_ok=True)\n",
    "selected_data, num_imp = load_data(input_dir, max_imp=1, regex='.*p_0.0_.*')\n",
    "train_base_names = list(train_base_names)\n",
    "np.random.shuffle(train_base_names)\n",
    "# select 3500\n",
    "train_base_names = set(train_base_names[:3500])\n",
    "assert len(train_base_names.intersection(val_base_names)) == 0, 'train and val base names overlap'\n",
    "\n",
    "train_code_map = {name.split('_')[2]: Catalogue.n_2_bn(name) for name in train_base_names}\n",
    "val_code_map = {name.split('_')[2]: Catalogue.n_2_bn(name) for name in val_base_names}\n",
    "\n",
    "with open(output_dir/'0imp_half/raw/train_base_names.txt', 'w') as f:\n",
    "    sorted_names = sorted(train_code_map.keys(), key=lambda x: int(x[1:]))\n",
    "    f.write('\\n'.join([train_code_map[name] for name in sorted_names]))\n",
    "with open(output_dir/'0imp_half/raw/val_base_names.txt', 'w') as f:\n",
    "    sorted_names = sorted(val_code_map.keys(), key=lambda x: int(x[1:]))\n",
    "    f.write('\\n'.join([val_code_map[name] for name in sorted_names]))\n",
    "\n",
    "training_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in train_base_names}\n",
    "validation_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in val_base_names}\n",
    "training_cat = Catalogue.from_dict(training_dict)\n",
    "validation_cat = Catalogue.from_dict(validation_dict)\n",
    "training_cat.to_file(output_dir/'0imp_half/raw/training_cat.lat')\n",
    "validation_cat.to_file(output_dir/'0imp_half/raw/validation_cat.lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 imp quarter (imperfection level 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 catalogue files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:13<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit cell catalogue with 1750 entries\n",
      "Unit cell catalogue with 1296 entries\n"
     ]
    }
   ],
   "source": [
    "Path(output_dir/'0imp_quarter/raw').mkdir(parents=True, exist_ok=True)\n",
    "selected_data, num_imp = load_data(input_dir, max_imp=1, regex='.*p_0.0_.*')\n",
    "train_base_names = list(train_base_names)\n",
    "np.random.shuffle(train_base_names)\n",
    "# select 1750\n",
    "train_base_names = set(train_base_names[:1750])\n",
    "assert len(train_base_names.intersection(val_base_names)) == 0, 'train and val base names overlap'\n",
    "\n",
    "train_code_map = {name.split('_')[2]: Catalogue.n_2_bn(name) for name in train_base_names}\n",
    "val_code_map = {name.split('_')[2]: Catalogue.n_2_bn(name) for name in val_base_names}\n",
    "\n",
    "with open(output_dir/'0imp_quarter/raw/train_base_names.txt', 'w') as f:\n",
    "    sorted_names = sorted(train_code_map.keys(), key=lambda x: int(x[1:]))\n",
    "    f.write('\\n'.join([train_code_map[name] for name in sorted_names]))\n",
    "with open(output_dir/'0imp_quarter/raw/val_base_names.txt', 'w') as f:\n",
    "    sorted_names = sorted(val_code_map.keys(), key=lambda x: int(x[1:]))\n",
    "    f.write('\\n'.join([val_code_map[name] for name in sorted_names]))\n",
    "    \n",
    "training_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in train_base_names}\n",
    "validation_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in val_base_names}\n",
    "training_cat = Catalogue.from_dict(training_dict)\n",
    "validation_cat = Catalogue.from_dict(validation_dict)\n",
    "print(training_cat)\n",
    "print(validation_cat)\n",
    "training_cat.to_file(output_dir/'0imp_quarter/raw/training_cat.lat')\n",
    "validation_cat.to_file(output_dir/'0imp_quarter/raw/validation_cat.lat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 imp (imperfection levels 0,2,4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 catalogue files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:00<00:00, 48.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base names: 8954\n",
      "35804\n",
      "Unit cell catalogue with 6994 entries\n",
      "Unit cell catalogue with 5184 entries\n"
     ]
    }
   ],
   "source": [
    "selected_data, num_imp = load_data(input_dir, max_imp=1, regex='.*p_0.0[247]?_.*')\n",
    "\n",
    "print(f'Number of base names: {len(num_imp)}')\n",
    "print(len(selected_data))\n",
    "\n",
    "training_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in train_base_names}\n",
    "validation_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in val_base_names}\n",
    "training_cat = Catalogue.from_dict(training_dict)\n",
    "validation_cat = Catalogue.from_dict(validation_dict)\n",
    "print(training_cat)\n",
    "print(validation_cat)\n",
    "Path(output_dir/'1imp/raw').mkdir(parents=True, exist_ok=True)\n",
    "training_cat.to_file(output_dir/'1imp/raw/training_cat.lat')\n",
    "validation_cat.to_file(output_dir/'1imp/raw/validation_cat.lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 imp (imperfection levels 0,2,4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 catalogue files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:38<00:00, 43.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base names: 8954\n",
      "62654\n",
      "Unit cell catalogue with 12238 entries\n",
      "Unit cell catalogue with 9072 entries\n"
     ]
    }
   ],
   "source": [
    "selected_data, num_imp = load_data(input_dir, max_imp=2, regex='.*p_0.0[247]?_.*')\n",
    "        \n",
    "print(f'Number of base names: {len(num_imp)}')\n",
    "print(len(selected_data))\n",
    "\n",
    "training_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in train_base_names}\n",
    "validation_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in val_base_names}\n",
    "training_cat = Catalogue.from_dict(training_dict)\n",
    "validation_cat = Catalogue.from_dict(validation_dict)\n",
    "print(training_cat)\n",
    "print(validation_cat)\n",
    "Path(output_dir/'2imp/raw').mkdir(parents=True, exist_ok=True)\n",
    "training_cat.to_file(output_dir/'2imp/raw/training_cat.lat')\n",
    "validation_cat.to_file(output_dir/'2imp/raw/validation_cat.lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 imp (imperfection levels 0,2,4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 catalogue files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:10<00:00, 50.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base names: 8954\n",
      "116354\n",
      "Unit cell catalogue with 22726 entries\n",
      "Unit cell catalogue with 16848 entries\n"
     ]
    }
   ],
   "source": [
    "selected_data, num_imp = load_data(input_dir, max_imp=4, regex='.*p_0.0[247]?_.*')\n",
    "\n",
    "print(f'Number of base names: {len(num_imp)}')\n",
    "print(len(selected_data))\n",
    "\n",
    "training_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in train_base_names}\n",
    "validation_dict = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in val_base_names}\n",
    "training_cat = Catalogue.from_dict(training_dict)\n",
    "validation_cat = Catalogue.from_dict(validation_dict)\n",
    "print(training_cat)\n",
    "print(validation_cat)\n",
    "Path(output_dir/'4imp/raw').mkdir(parents=True, exist_ok=True)\n",
    "training_cat.to_file(output_dir/'4imp/raw/training_cat.lat')\n",
    "validation_cat.to_file(output_dir/'4imp/raw/validation_cat.lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 catalogue files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:25<00:00, 29.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base names: 8950\n",
      "Unit cell catalogue with 3888 entries\n"
     ]
    }
   ],
   "source": [
    "# test data is imperfections of the validation data\n",
    "selected_data, num_imp = load_data(input_dir, max_imp=3, regex='.*p_0.1_.*')\n",
    "selected_test_data = {name:selected_data[name] for name in selected_data if Catalogue.n_2_bn(name) in val_base_names}\n",
    "        \n",
    "print(f'Number of base names: {len(num_imp)}')\n",
    "test_cat = Catalogue.from_dict(selected_test_data)\n",
    "print(test_cat)\n",
    "test_cat.to_file(output_dir/'0imp/raw/test_cat.lat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLAMMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
